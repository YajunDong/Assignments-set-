import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk

from sklearn.metrics import mean_squared_error
from statsmodels.tsa.arima.model import ARIMA
from nltk.sentiment.vader import SentimentIntensityAnalyzer

nltk.download('vader_lexicon')

def Station1_loadData():
    # import .CSV file
    df1 = pd.read_csv("E:\\NeoBank\\Weather (2).csv", engine='python', encoding='unicode_escape')

    # Removes the rows that contain NULL values
    df1.dropna(axis=0, how='any', subset=None, inplace=True)

    # Basic information
    nRow, nCol = df1.shape
    print(f'There are {nRow} rows and {nCol} columns')
    print(df1.head(5))
    print(df1.tail(5))
    print(df1.info())
    return df1

def Station2_featuresEngineeringBase(df1):
    # Weather basic features trend
    plt.plot(df1['Date'], df1['Minimum temperature (°C)'], label='MinTemp')
    plt.plot(df1['Date'], df1['Maximum temperature (°C)'], label='MaxTemp')
    plt.bar(df1['Date'], df1['Rainfall (mm)'], label='Rain')
    plt.plot(df1['Date'], df1['9am Temperature (°C)'], label='9amTemp')
    plt.plot(df1['Date'], df1['3pm Temperature (°C)'], label='3pmTemp')
    plt.legend(loc='upper right')
    plt.show()

    # Calculate the average temperature between minimum and maximum temperature.
    # Then show and compare the average temperature and rainfall trend during the time.
    df1['AvgTemp'] = (df1['Minimum temperature (°C)'] + df1['Maximum temperature (°C)']) / 2
    print(df1['AvgTemp'])
    features_considered = ['AvgTemp', 'Rainfall (mm)']
    features = df1[features_considered]
    features.index = df1['Date']
    print(features.head())
    features.plot(subplots=True)
    plt.show()

    # Normalize the data streams
    dataset = features.values
    data_mean = dataset.mean(axis=0)
    data_std = dataset.std(axis=0)
    print(data_mean)
    print(data_std)
    dataset = (dataset - data_mean) / data_std
    print(dataset[:5])
    plt.plot(dataset)
    plt.show()

    # Covariance matrix between the average temperature and the rainfall
    array1 = df1['AvgTemp'].astype(float)
    array2 = df1['Rainfall (mm)'].astype(float)
    Average_temperature_data = np.array([array1])
    Rainfall_data = np.array([array2])
    cov_matrix = np.cov(Average_temperature_data, Rainfall_data)
    cov_df1 = pd.DataFrame(cov_matrix, columns=['AvgTemp', 'Rainfall (mm)'], index=['AvgTemp', 'Rainfall (mm)'])
    plt.figure(figsize=(8, 6))
    sns.heatmap(cov_df1, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
    plt.title('Covariance Matrix Heatmap (Average Temperature vs. Rainfall (mm))')
    plt.show()

    return df1

# Station #3 Model Design
class Station3_modelDesign():
    def ARIMA(self, data, tt_ratio, column_name):
        X = data[column_name].values
        size = int(len(X) * tt_ratio)
        train, test = X[0:size], X[size:len(X)]
        history = [x for x in train]
        predictions = list()
        dates = data.index[size:len(X)]  # Dates corresponding to the test set
        for t in range(len(test)):
            model = ARIMA(history, order=(5, 1, 0))
            model_fit = model.fit()
            output = model_fit.forecast()
            yhat = output[0]
            predictions.append(yhat)
            obs = test[t]
            history.append(obs)
            print('progress:%', round(100 * (t / len(test))), '\t predicted=%f, expected=%f' % (yhat, obs), end="\r")
        error = mean_squared_error(test, predictions)
        print('\n Test MSE: %.3f' % error)

        # Create a new DataFrame to store the predictions with corresponding dates
        predictions_df1 = pd.DataFrame({'Date': dates, 'Predicted Data': predictions})
        predictions_df1.set_index('Date', inplace=True)

        # Plot original data and the predicted data
        plt.plot(data[column_name], color='blue', linewidth=2, label=f"Original {column_name}")
        plt.plot(predictions_df1, color='green', linewidth=3, label=f"Predicted {column_name}")
        plt.axvline(x=dates[0], linewidth=5, color='red')

        # Labeling the graph
        plt.title(f"Original and Predicted {column_name} using ARIMA Model")
        plt.xlabel("Date")
        plt.ylabel(f"{column_name}")
        plt.legend()
        plt.show()

def main():
    # Station #1
    df1_station1 = Station1_loadData()
    # Station #2
    df1_station2 = Station2_featuresEngineeringBase(df1_station1)
    print(df1_station2)

    # Station #4 Product Implementation (Implementing Station #3 Model Design)
    tt_ratio = 0.8  # Train to Test ratio
    column_name = 'AvgTemp'
    model_design = Station3_modelDesign()
    model_design.ARIMA(data=df1_station2, tt_ratio=tt_ratio, column_name=column_name)

if __name__ == '__main__':
    main()


# Client information
def Station1_loadData():
    # Get the names of sheets to be read
    sheet_names_to_read = ["1", "2", "3", "4", "5"]

    # Loop through each sheet and convert it to a separate .csv file
    for sheet_name in sheet_names_to_read:
        df_sheet = pd.read_excel('E:\\NeoBank\\Client_Cash_Accounts (2).xlsx', sheet_name)
        csv_filename = f"E:\\NeoBank\\{sheet_name}.csv"
        df_sheet.to_csv(csv_filename, index=False)

    # Read the CSV files and combine them into a single DataFrame
    filenames = [f"E:\\NeoBank\\{sheet_name}.csv" for sheet_name in sheet_names_to_read]
    df2 = pd.concat([pd.read_csv(file, encoding='unicode_escape', delimiter=',') for file in filenames])

    # Removes the rows that contain NULL values
    df2.dropna(axis=0, how='any', subset=None, inplace=True)

    # Basic information
    nRow, nCol = df2.shape
    print(f'There are {nRow} rows and {nCol} columns')
    print(df2.head(5))
    print(df2.tail(5))
    print(df2.info())

    return df2

def Station2_featuresEngineeringBase(df2):
    # Show the cash flow and balance trend based on transaction date
    features_considered = ['Flow', 'Balance', 'Client']
    features = df2[features_considered]
    features.index = df2['Transaction Date']

    # Group the data by the "Client" column
    grouped_features = features.groupby('Client')

    # Plot the trends for flow and balance for each client separately
    for client, data in grouped_features:
        data.drop(columns=['Client'], inplace=True)  # Drop the "Client" column to avoid redundant plots
        data.plot(subplots=True, title=f"Client {client}")
        plt.show()

    return df2

# Station #3 Model Design
class Station3_modelDesign():
    def ARIMA(self, data, tt_ratio, column_name, features):
        # Group the data by the "Client" column
        grouped_features = features.groupby('Client')

        for client, data in grouped_features:
            # Extract the data for the specific client
            client_data = data[[column_name]]
            X = client_data[column_name].values
            size = int(len(X) * tt_ratio)
            train, test = X[0:size], X[size:len(X)]
            history = [x for x in train]
            predictions = list()
            dates = client_data.index[size:len(X)]  # Dates corresponding to the test set
            for t in range(len(test)):
                model = ARIMA(history, order=(5, 1, 0))
                model_fit = model.fit()
                output = model_fit.forecast()
                yhat = output[0]
                predictions.append(yhat)
                obs = test[t]
                history.append(obs)
                print(f'Client {client}: progress: %{round(100 * (t / len(test))), 2}\t predicted={yhat}, expected={obs}', end="\r")
            print(f'\nClient {client}: Test MSE: %.3f' % mean_squared_error(test, predictions))

            # Create a new DataFrame to store the predictions with corresponding dates
            predictions_df = pd.DataFrame({'Date': dates, f'Predicted {column_name}': predictions})
            predictions_df.set_index('Date', inplace=True)

            # Plot original data and the predicted data
            plt.plot(client_data[column_name], color='blue', linewidth=2, label=f"Original {column_name}")
            plt.plot(predictions_df, color='green', linewidth=3, label=f"Predicted {column_name}")
            plt.axvline(x=dates[0], linewidth=5, color='red')

            # Labeling the graph
            plt.title(f"Client {client} - Original and Predicted {column_name} using ARIMA Model")
            plt.xlabel("Date")
            plt.ylabel(f"{column_name}")
            plt.legend()
            plt.show()

def main():
    # Station #1
    df2_station1 = Station1_loadData()
    # Station #2
    df2_station2 = Station2_featuresEngineeringBase(df2_station1)

    # Station #4 Product Implementation (Implementing Station #3 Model Design)
    tt_ratio = 0.8  # Train to Test ratio
    column_name = 'Balance'
    model_design = Station3_modelDesign()
    model_design.ARIMA(data=df2_station2, tt_ratio=tt_ratio, column_name=column_name, features=df2_station2)

if __name__ == "__main__":
    main()


# Corn price history
def Station1_loadData():
    # Import .xlsx file
    df3 = pd.read_excel('E:\\NeoBank\\CORN_PriceHistory (4).xlsx')

    # Convert and save to .csv file
    df3.to_csv('E:\\NeoBank\\CORN_PriceHistory (4).csv', index=False)
    df3 = pd.read_csv("E:\\NeoBank\\CORN_PriceHistory (4).csv", encoding='unicode_escape', delimiter=',', header=15)

    # Remove the Unnamed columns
    df3.drop(columns=['Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], inplace=True)
    print(df3.columns)

    # Removes the rows that contain NULL values
    df3.dropna(axis=0, how='any', subset=None, inplace=True)

    # Basic information
    nRow, nCol = df3.shape
    print(f'There are {nRow} rows and {nCol} columns')
    print(df3.head(5))
    print(df3.tail(5))
    print(df3.info())
    print(df3.columns)

    # Convert the elements to float
    df3['Last'] = df3['Last'].astype(float)
    df3['Open Interest'] = df3['Open Interest'].astype(float)
    df3['Settlement Price'] = df3['Settlement Price'].astype(float)
    df3['CVol'] = df3['CVol'].astype(float)
    df3['Ask'] = df3['Ask'].astype(float)
    df3['Bid'] = df3['Bid'].astype(float)
    df3['High'] = df3['High'].astype(float)
    df3['Low'] = df3['Low'].astype(float)

    # Sort the date from oldest to newest
    df3.sort_values(by='Date', inplace=True)
    df3.reset_index(drop=True, inplace=True)

    return df3

def Station2_featuresEngineeringBase(df3):
    # Show the last price and the open interest changing trend over time.
    features_considered = ['Last', 'Open Interest']
    features = df3[features_considered]
    features.index = df3['Date']
    print(features.head(15))
    features.plot(subplots=True)
    plt.show()

    # Normalize the data streams
    dataset = features.values
    data_mean = dataset.mean(axis=0)
    data_std = dataset.std(axis=0)
    print(data_mean)
    print(data_std)
    dataset = (dataset - data_mean) / data_std
    print(dataset[:5])
    plt.plot(dataset)
    plt.show()

    # Show the settlement price and the trading volume changing trend over time
    features_considered = ['Settlement Price', 'CVol']
    features = df3[features_considered]
    features.index = df3['Date']
    print(features.head())
    features.plot(subplots=True)
    plt.show()

    # Calculate the spread
    df3['spread'] = (df3['Ask'] - df3['Bid'])
    print(df3['spread'])
    plt.plot(df3['Date'], df3['spread'], label='spread')
    plt.show()

    # Calculate the difference between the High and Low Price, which reflects the price volatility during the trading day.
    df3['price_volatility'] = (df3['High'] - df3['Low'])
    print(df3['price_volatility'])
    plt.plot(df3['Date'], df3['price_volatility'], label='Price volatility')
    plt.show()

    # The logarithmic return for the corresponding time step in the "Settlement Price" time series
    rets = np.log(df3['Settlement Price'] / df3['Settlement Price'].shift(1))
    print(rets)
    plt.plot(df3['Date'], rets, label='rets')
    plt.show()

    return df3

# Station #3 Model Design
class Station3_modelDesign():
    def ARIMA(self, data, tt_ratio, column_name):
        X = data[column_name].values
        size = int(len(X) * tt_ratio)
        train, test = X[0:size], X[size:len(X)]
        history = [x for x in train]
        predictions = list()
        dates = data.index[size:len(X)]  # Dates corresponding to the test set
        for t in range(len(test)):
            model = ARIMA(history, order=(5, 1, 0))
            model_fit = model.fit()
            output = model_fit.forecast()
            yhat = output[0]
            predictions.append(yhat)
            obs = test[t]
            history.append(obs)
            print('progress:%', round(100 * (t / len(test))), '\t predicted=%f, expected=%f' % (yhat, obs), end="\r")
        error = mean_squared_error(test, predictions)
        print('\n Test MSE: %.3f' % error)

        # Create a new DataFrame to store the predictions with corresponding dates
        predictions_df3 = pd.DataFrame({'Date': dates, 'Predicted Data': predictions})
        predictions_df3.set_index('Date', inplace=True)

        # Plot original data and the predicted data
        plt.plot(data[column_name], color='blue', linewidth=2, label=f"Original {column_name}")
        plt.plot(predictions_df3, color='green', linewidth=3, label=f"Predicted {column_name}")
        plt.axvline(x=dates[0], linewidth=5, color='red')  # Using the last date of the reversed test set

        # Labeling the graph
        plt.title(f"Original and Predicted {column_name} using ARIMA Model")
        plt.xlabel("Date")
        plt.ylabel(f"{column_name}")
        plt.legend()
        plt.show()

def main():
    # Station #1
    df3_station1 = Station1_loadData()
    # Station #2
    df3_station2 = Station2_featuresEngineeringBase(df3_station1)
    print(df3_station2)

    # Station #4 Product Implementation (Implementing Station #3 Model Design)
    tt_ratio = 0.8  # Train to Test ratio
    column_name = 'CVol'
    model_design = Station3_modelDesign()
    model_design.ARIMA(data=df3_station2, tt_ratio=tt_ratio, column_name=column_name)

if __name__ == '__main__':
    main()


# Wheat price history
def Station1_loadData():
    # import the data of wheat history price
    df4 = pd.read_excel('E:\\NeoBank\\WHEAT_PriceHistory (2).xlsx')

    # Convert and save to .csv file
    df4.to_csv('E:\\NeoBank\\WHEAT_PriceHistory (2).csv', index=False)
    df4 = pd.read_csv("E:\\NeoBank\\WHEAT_PriceHistory (2).csv", encoding='unicode_escape', delimiter=',', header=15)

    # Remove the Unnamed columns
    df4.drop(columns=['Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14'], inplace=True)
    print(df4.columns)

    # removes the rows that contain NULL values
    df4.dropna(axis=0, how='any', subset=None, inplace=True)

    # Convert the elements to float
    df4['Last'] = df4['Last'].astype(float)
    df4['Open Interest'] = df4['Open Interest'].astype(float)
    df4['Settlement Price'] = df4['Settlement Price'].astype(float)
    df4['CVol'] = df4['CVol'].astype(float)
    df4['Ask'] = df4['Ask'].astype(float)
    df4['Bid'] = df4['Bid'].astype(float)
    df4['High'] = df4['High'].astype(float)
    df4['Low'] = df4['Low'].astype(float)

    # Sort the date from oldest to newest
    df4.sort_values(by='Date', inplace=True)
    df4.reset_index(drop=True, inplace=True)

    # basic information
    nRow, nCol = df4.shape
    print(f'There are {nRow} rows and {nCol} columns')
    print(df4.head(5))
    print(df4.tail(5))
    print(df4.info())
    print(df4.columns)

    return df4

def Station2_featuresEngineeringBase(df4):
    # Show the last price and the open interest changing trend over time
    features_considered = ['Last', 'Open Interest']
    features = df4[features_considered]
    features.index = df4['Date']
    print(features.head())
    features.plot(subplots=True)
    plt.show()

    # Normalize the data streams
    dataset = features.values
    data_mean = dataset.mean(axis=0)
    data_std = dataset.std(axis=0)
    print(data_mean)
    print(data_std)
    dataset = (dataset - data_mean) / data_std
    print(dataset[:5])
    plt.plot(dataset)
    plt.show()

    # Show the settlement price and the trading volume changing trend over time
    features_considered = ['Settlement Price', 'CVol']
    features = df4[features_considered]
    features.index = df4['Date']
    print(features.head())
    features.plot(subplots=True)
    plt.show()

    # Calculate the spread
    df4['spread'] = (df4['Ask'] - df4['Bid'])
    print(df4['spread'])
    plt.plot(df4['Date'], df4['spread'], label='spread')
    plt.show()

    # Calculate the difference between the High and Low Price, which reflects the price volatility during the trading day.
    df4['price_volatility'] = (df4['High'] - df4['Low'])
    print(df4['price_volatility'])
    plt.plot(df4['Date'], df4['price_volatility'], label='Price volatility')
    plt.show()

    # The logarithmic return for the corresponding time step in the "Settlement Price" time series
    rets = np.log(df4['Settlement Price'] / df4['Settlement Price'].shift(1))
    print(rets)
    plt.plot(df4['Date'], rets, label='rets')
    plt.show()

    return df4

# Station #3 Model Design
class Station3_modelDesign():
    def ARIMA(self, data, tt_ratio, column_name):
        X = data[column_name].values
        size = int(len(X) * tt_ratio)
        train, test = X[0:size], X[size:len(X)]
        history = [x for x in train]
        predictions = list()
        dates = data.index[size:len(X)]  # Dates corresponding to the test set
        for t in range(len(test)):
            model = ARIMA(history, order=(5, 1, 0))
            model_fit = model.fit()
            output = model_fit.forecast()
            yhat = output[0]
            predictions.append(yhat)
            obs = test[t]
            history.append(obs)
            print('progress:%', round(100 * (t / len(test))), '\t predicted=%f, expected=%f' % (yhat, obs), end="\r")
        error = mean_squared_error(test, predictions)
        print('\n Test MSE: %.3f' % error)

        # Create a new DataFrame to store the predictions with corresponding dates
        predictions_df4 = pd.DataFrame({'Date': dates, 'Predicted Data': predictions})
        predictions_df4.set_index('Date', inplace=True)

        # Plot original data and the predicted data
        plt.plot(data[column_name], color='blue', linewidth=2, label=f"Original {column_name}")
        plt.plot(predictions_df4, color='green', linewidth=3, label=f"Predicted {column_name}")
        plt.axvline(x=dates[0], linewidth=5, color='red')  # Using the last date of the reversed test set

        # Labeling the graph
        plt.title(f"Original and Predicted {column_name} using ARIMA Model")
        plt.xlabel("Date")
        plt.ylabel(f"{column_name}")
        plt.legend()
        plt.show()

def main():
    # Station #1
    df4_station1 = Station1_loadData()
    # Station #2
    df4_station2 = Station2_featuresEngineeringBase(df4_station1)
    print(df4_station2)

    # Station #4 Product Implementation (Implementing Station #3 Model Design)
    tt_ratio = 0.8  # Train to Test ratio
    column_name = 'CVol'
    model_design = Station3_modelDesign()
    model_design.ARIMA(data=df4_station2, tt_ratio=tt_ratio, column_name=column_name)

if __name__ == '__main__':
    main()


# Unstructured data: Commodity news
# Import the json file
df = pd.read_json('E:\\NeoBank\\Commodity_News.json')
df.columns = ['Date', 'Source', 'Headline']

# Instantiate the sentiment intensity analyzer
vader = SentimentIntensityAnalyzer()

# Iterate through the headlines and get the polarity scores using vader
scores = df['Headline'].apply(vader.polarity_scores).tolist()

# Convert the 'scores' list of dicts into a DataFrame
scores_df = pd.DataFrame(scores)

# Join the DataFrames of the news and the list of dicts
df = df.join(scores_df, rsuffix='_right')

# Drop rows with missing sentiment scores
df.dropna(subset=['neg', 'neu', 'pos', 'compound'], inplace=True)

# Convert the sentiment score columns to numeric types and handle problematic values
df[['neg', 'neu', 'pos', 'compound']] = df[['neg', 'neu', 'pos', 'compound']].apply(pd.to_numeric, errors='coerce')

# Drop any remaining rows with NaN values after conversion
df.dropna(subset=['neg', 'neu', 'pos', 'compound'], inplace=True)

# Save the updated DataFrame with sentiment scores
df.to_json('E:\\NeoBank\\vader_scores.json')

# Convert the 'Date' column to datetime format
date_format = '%A, %B %d, %Y %I:%M:%S %p (%Z)'
df['Date'] = pd.to_datetime(df['Date'], format=date_format)

# Check the data types after conversion
print("Data types after date conversion:")
print(df['Date'].dtypes)

# Group by date and calculate the mean sentiment scores for numeric columns only
mean_scores = df.groupby(['Date']).mean(numeric_only=True)
print(mean_scores)

# Check DataFrame columns and keep only numeric columns
mean_scores = mean_scores.select_dtypes(include=['float64'])

# Boxplot of time-series of sentiment score for each date
mean_scores.plot(kind = 'box')
plt.grid()
plt.title('Box plot of sentiment score overtime')
plt.show()

# get the lastest month sentiment score as adjustment
lastest_month_score = mean_scores.tail(20).mean()
lastest_month_score.fillna(0,inplace=True)
lastest_month_score.plot(kind='bar')
plt.title('Sentiment Score')
plt.grid()
plt.show()

# adjusted weights - based on relative strength of sentiment score
average_score = lastest_month_score.mean()
lastest_month_score = (lastest_month_score -average_score)/len(lastest_month_score)
lastest_month_score.plot(kind='bar')
plt.title('Adjustment on Portfolio Weights -- Sentiment Score')
plt.grid()
print(lastest_month_score)
plt.show()
